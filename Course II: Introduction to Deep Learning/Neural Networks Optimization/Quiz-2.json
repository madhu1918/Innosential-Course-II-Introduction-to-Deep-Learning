[{
	"title": "Quiz",
	"activityType": "quiz",
	"questions": [{
			"questionType": "mcq",
			"questionText": [{
				"text": "If you have 10,000,000 examples, how would you split the train/dev/test set?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "33% train . 33% dev . 33% test",
					"correct": false
				},

				{
					"option": "98% train . 1% dev . 1% test",
					"correct": "True"
				},

				{
					"option": "60% train . 20% dev . 20% test",
					"correct": false
				}
			],
			"correct_feedback": "Correct",
			"wrong_feedback": "Incorrect."
		},

		{
			"questionType": "mcq",
			"questionText": [{
					"text": "The dev and test set should:"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "Come from the same distribution",
					"correct": "True"
				},

				{
					"option": "Come from di erent distributions",
					"correct": false
				},

				{
					"option": "identical each other (same (x,y) pairs)",
					"correct": false
				},

				{
					"option": "Have the same number of examples",
					"correct": false
				}


			],
			"correct_feedback": "Correct",
			"wrong_feedback": "Incorrect"
		},



		{
			"questionType": "checkbox",
			"questionText": [{
				"text": "If your Neural Network model seems to have high variance, what of the following would be promising things to try?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "Add regularization",
					"correct": "True"
				},

				{
					"option": "Make the Neural Network deeper",
					"correct": "True"
				},
				{
					"option": "Get more training data",
					"correct": "True"
				},
				{
					"option": "Increase the number of units in each hidden layer",
					"correct": "True"
				},
				{
					"option": "Get more test data",
					"correct": "True"
				}

			],
			"correct_feedback": "correct",
			"wrong_feedback": "Incorrect"
		},



		{
			"questionType": "checkbox",
			"questionText": [{ 
					"text": "You are working on an learning automated check-out kiosk for a supermarket, and are building a classi er for apples, bananas and oranges. Suppose your classi er obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classi er? (Check all that apply.)"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "Increase the regularization parameter lambda",
					"correct": "True"
				},

				{
					"option": "Decrease the regularization parameter lambda",
					"correct": "True"
				},
				{
					"option": "Get more training data",
					"correct": "True"
				},
				{
					"option": "Use a bigger neural network",
					"correct": "True"
				}

			],
			"correct_feedback": "Correct",
			"wrong_feedback": "Incorrect"
		},



		{
			"questionType": "mcq",
			"questionText": [{
					"text": "What is weight decay?"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.",
					"correct": "True"
				},

				{
					"option": "Gradual corruption of the weights in the neural network if it is trained on noisy data.",
					"correct": false
				},
				{
					"option": "The process of gradually decreasing the learning rate during training.",
					"correct": false
				},
				{
					"option": "A technique avoid vanishing gradient by imposing a ceiling on the values of the weights.",
					"correct": false
				}

			],
			"correct_feedback": "Correct",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "mcq",
			"questionText": [{
				"text": "What happens when you increase the regularization hyperparameter lambda?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "Weights are pushed toward becoming smaller (closer to 0)",
					"correct": "True"
				},

				{
					"option": "Weights are pushed toward becoming bigger (further from 0)",
					"correct": false
				},
				{
					"option": "Doubling lambda should roughly result in doubling the weights",
					"correct": false
				},
				{
					"option": "Gradient descent taking bigger steps with each iteration (proportional to lambda)",
					"correct": false
				}

			],
			"correct_feedback": "Correct ",
			"wrong_feedback": "Incorrect"
		}, {
			"questionType": "mcq",
			"questionText": [{
					"text": "With the inverted dropout technique, at test time:"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "You do not apply dropout (do not randomly eliminate units), but keep the 1/keep_prob factor in the calculations used in training.",
					"correct": false
				},

				{
					"option": "You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training",
					"correct": "True"
				},
				{
					"option": "You apply dropout (randomly eliminating units) and do not keep the 1/keep_prob factor in the calculations used in training",
					"correct": false
				},
				{
					"option": "You apply dropout (randomly eliminating units) but keep the 1/keep_prob factor in the calculations used in training.",
					"correct": false
				}

			],
			"correct_feedback": "Correct.",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "checkbox",
			"questionText": [{
					"text": "Increasing the parameter keep_prob from (say) 0.5 to 0.6 will likely cause the following: (Check the two that apply)"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "Increasing the regularization effect",
					"correct": "True"
				},

				{
					"option": "Reducing the regularization e ect",
					"correct": "True"
				},
				{
					"option": "Causing the neural network to end up with a higher training set error",
					"correct": "True"
				},
				{
					"option": "Causing the neural network to end up with a lower training set error",
					"correct": "True"
				}

			],
			"correct_feedback": "Correct.",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "checkbox",
			"questionText": [{
					"text": "Which of these techniques are useful for reducing variance (reducing over tting)? (Check all that apply.)"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "Data augmentation",
					"correct": "True"
				},

				{
					"option": "Dropout",
					"correct": "True"
				},
				{
					"option": "Gradient Checking.",
					"correct": "True"
				},
				{
					"option": "Vanishing gradient",
					"correct": "True"
				},
				{
					"option": "Xavier initialization",
					"correct": "True"
				},
				{
					"option": "L2 regularization",
					"correct": "True"
				},
				{
					"option": "Exploding gradient",
					"correct": "True"
				}

			],
			"correct_feedback": "Correct.",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "mcq",
			"questionText": [{
					"text": "Why do we normalize the inputs x ?"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "t makes it easier to visualize the data",
					"correct": false
				},

				{
					"option": "It makes the parameter initialization faster",
					"correct": false
				},
				{
					"option": "It makes the cost function faster to optimize",
					"correct": "True"
				}
			],
			"correct_feedback": "Correct.",
			"wrong_feedback": "Incorrect"
		}
	]
}]