[{
	"title": "Quiz",
	"activityType": "quiz",
	"questions": [{
			"questionType": "mcq",
			"questionText": [{
				"text": "What is the \"cache\" used for in our implementation of forward propagation and backward propagation?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "It is used to keep track of the hyperparameters that we are searching over, to speed up computation.",
					"correct": false
				},

				{
					"option": "We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.",
					"correct": true
				},

				{
					"option": "We use it to pass variables computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations.",
					"correct": false
				},

				{
					"option": "It is used to cache the intermediate values of the cost function during training.",
					"correct": false
				}

			],
			"correct_feedback": "Correct, the \"cache\" records values from the forward propagation units and sends it to the backward propagation units because it is needed to compute the chain rule derivatives.",
			"wrong_feedback": "Incorrect"
		},

		{
			"questionType": "checkbox",
			"questionText": [{
				"text": "Among the following, which ones are \"hyperparameters\"? (Check all that apply.)"
			}],
			"max_marks": "1",
			"options": [{
					"option": "activation values a<sup>[l]</sup>",
					"correct": true
				},

				{
					"option": "weight matrices W<sup>[l]</sup>",
					"correct": true
				},

				{
					"option": "number of layers L in the neural network",
					"correct": true
				},

				{
					"option": "learning rate α",
					"correct": true
				},
				{
					"option": "number of iterations ",
					"correct": true
				},
				{
					"option": "size of the hidden layers n<sup>[l]</sup>",
					"correct": true
				},
				{
					"option": "bias vectors b<sup>[l]</sup>",
					"correct": true
				}


			],
			"correct_feedback": "correct",
			"wrong_feedback": "Incorrect"
		},



		{
			"questionType": "mcq",
			"questionText": [{
				"text": "Which of the following statements is true?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.",
					"correct": true
				},

				{
					"option": "The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers.",
					"correct": false
				}

			],
			"correct_feedback": "correct",
			"wrong_feedback": "Incorrect"
		},



		{
			"questionType": "mcq",
			"questionText": [{
				"text": "Vectorization allows you to compute forward propagation in an L -layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers l=1, 2, ...,L. True/False?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "True",
					"correct": false
				},

				{
					"option": "False",
					"correct": true
				}

			],
			"correct_feedback": "Correct Forward propagation propagates the input through the layers, although for shallow networks we may just write all the lines ( a <sup>[2]</sup>= g <sup>[2]</sup>( z sup[2]</sup>, ) z <sup>[2]</sup>= W <sup>[2][1]</sup>a<sup>[2]</sup>+ b, ...)in a deeper network, we cannot avoid a for loop iterating over the layers: (<sup>[l]</sup>a= g<sup>[l]</sup>( z<sup>[l]</sup>,) z<sup>[l]</sup>= W<sup>[l][l−1]</sup>a<sup>[l]</sup>+ b, ...).",
			"wrong_feedback": "Incorrect"
		},



		{
			"questionType": "mcq",
			"questionText": [{
					"text": "Assume we store the values for n [l] in an array called layers, as follows: layer_dims = [ n x , 4,3,2,1]. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model?"
				},
				{
					"image": {
						"imageSRC": "image-1.png",
						"imageName": "Figure-1"
					}
				},
				{
					"image": {
						"imageSRC": "image-2.png",
						"imageName": "Figure-2"
					}
				},
				{
					"image": {
						"imageSRC": "image-3.png",
						"imageName": "Figure-3"
					}
				},
				{
					"image": {
						"imageSRC": "image-4.png",
						"imageName": "Figure-4"
					}
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "Figure-1",
					"correct": false
				},

				{
					"option": "Figure 2:",
					"correct": false
				},

				{
					"option": "Figure 3:",
					"correct": false
				},
				{
					"option": "Figure 4:",
					"correct": true
				}



			],
			"correct_feedback": "Correct",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "mcq",
			"questionText": [{
					"text": "Consider the following neural network."
				},
				{
					"image": {
						"imageSRC": "image-5.png",
						"imageName": ""
					}
				}, {
					"text": "How many layers does this network have?"
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "The number of layers L is 4. The number of hidden layers is 3.",
					"correct": true
				},

				{
					"option": "The number of layers L is 3. The number of hidden layers is 3.",
					"correct": false
				},
				{
					"option": "The number of layers L is 4. The number of hidden layers is 4.",
					"correct": false
				},
				{
					"option": "The number of layers L is 5. The number of hidden layers is 4.",
					"correct": false
				}

			],
			"correct_feedback": "Correct Yes. As seen in lecture, the number of layers is counted as the number of hidden layers + 1. The input and output layers are not counted as hidden layers.",
			"wrong_feedback": "Incorrect"
		}, {
			"questionType": "mcq",
			"questionText": [{
				"text": "During forward propagation, in the forward function for a layer l you need to know what is the activation function in a layer Networks (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer l , since the gradient depends on it. True/False?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "True",
					"correct": true
				},

				{
					"option": "False",
					"correct": false
				}

			],
			"correct_feedback": "Yes, as you've seen in the week 3 each activation has a di erent derivative. Thus, during backpropagation you need to know which activation was used in the forward propagation to be able to compute the correct derivative.",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "checkbox",
			"questionText": [{
				"text": "There are certain functions with the following properties: <br><br>(i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially smaller network. True/False?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "True",
					"correct": true
				},

				{
					"option": "False",
					"correct": false
				}


			],
			"correct_feedback": "Correct.",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "checkbox",
			"questionText": [{
					"text": "Consider the following 2 hidden layer neural network:"
				},
				{
					"image": {
						"imageSRC": "image-6.png",
						"imageName": ""
					}
				},
				{
					"text": "Which of the following statements are True? (Check all that apply)."
				}
			],
			"max_marks": "1",
			"options": [{
					"option": "W<sup>[1]</sup> will have shape (4, 4)",
					"correct": true
				},

				{
					"option": "b<sup>[1]</sup> will have shape (4, 1)",
					"correct": true
				},
				{
					"option": "W<sup>[1]</sup> will have shape (3, 4)",
					"correct": true
				},

				{
					"option": "b<sup>[1]</sup> will have shape (3, 1)",
					"correct": true
				},
				{
					"option": "W<sup>[2]</sup> will have shape (3, 4)",
					"correct": true
				},
				{
					"option": "b<sup>[2]</sup> will have shape (1, 1)",
					"correct": true
				},
				{
					"option": "W<sup>[2]</sup> will have shape (3, 1)",
					"correct": true
				},
				{
					"option": "b<sup>[2]</sup> will have shape (3, 1)",
					"correct": true
				},
				{
					"option": "W<sup>[3]</sup> will have shape (3, 1)",
					"correct": true
				},
				{
					"option": "b<sup>[3]</sup> will have shape (1, 1)",
					"correct": true
				},
				{
					"option": "W<sup>[3]</sup> will have shape (1, 3)",
					"correct": true
				},
				{
					"option": "b<sup>[3]</sup> will have shape (3, 1)",
					"correct": true
				}

			],
			"correct_feedback": "Correct",
			"wrong_feedback": "Incorrect"
		},
		{
			"questionType": "mcq",
			"questionText": [{
				"text": "Whereas the previous question used a speci c network, in the general case what is the dimension of W^{[l]}, the weight matrix associated with layer l ?"
			}],
			"max_marks": "1",
			"options": [{
					"option": "W<sup>[l]</sup> have shape (n<sup>[l-1]</sup>, n<sup>[l]</sup>)",
					"correct": false
				},

				{
					"option": "W<sup>[l]</sup> have shape (n<sup>[l+1]</sup>, n<sup>[l]</sup>)",
					"correct": false
				},
				{
					"option": "W<sup>[l]</sup> have shape (n<sup>[l]</sup>, n<sup>[l-1]</sup>)",
					"correct": true
				},
				{
					"option": "W<sup>[l]</sup> have shape (n<sup>[l]</sup>, n<sup>[l+1]</sup>)",
					"correct": false
				}

			],
			"correct_feedback": "Correct. True",
			"wrong_feedback": "Incorrect"
		}
	]
}]